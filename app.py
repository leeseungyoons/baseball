import streamlit as st
import re
from sentiment_model import SentimentAnalyzer
from keyword_extractor import KeywordExtractor
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
from collections import Counter
from wordcloud import WordCloud

# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
sa = SentimentAnalyzer()
ke = KeywordExtractor()

st.title("âš¾ ìŠ¤í¬ì¸  ê¸°ì‚¬ ë¶„ì„ ì‹œìŠ¤í…œ")

uploaded_file = st.file_uploader("ğŸ“° ë‰´ìŠ¤ë‚˜ ì¤‘ê³„ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”. (.txt)", type="txt")

if uploaded_file is not None:
    text = uploaded_file.read().decode("utf-8")
    articles = re.split(r'(?:\n\s*){5,}', text.strip())  # 5ì¤„ ì´ìƒ ë¹ˆì¤„ ê¸°ì¤€ ë¶„ë¦¬

    sentiment_counts = Counter({"ê¸ì •": 0, "ë¶€ì •": 0})
    all_keywords = Counter()

    st.subheader("ğŸ“„ ê¸°ì‚¬ ë¶„ì„ ê²°ê³¼")
    label_map = {"Positive": "ê¸ì •", "Negative": "ë¶€ì •"}

    # âœ… ìŠ¤í¬ì¸  ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ
    positive_words = [
        "ìŠ¹ë¦¬", "ì™„ìŠ¹", "ëŒ€ìŠ¹", "ì••ìŠ¹", "ì´ê²¼ë‹¤", "ì´ê¸°ë©°", "ì—­ì „ìŠ¹", "ëë‚´ê¸°",
        "ìš°ìŠ¹", "ì—°ìŠ¹", "í™ˆëŸ°", "ë©€í‹°íˆíŠ¸", "3ì•ˆíƒ€", "4ì•ˆíƒ€", "ì¾Œì¡°", "í˜¸íˆ¬",
        "í˜¸ì„±ì ", "í™œì•½", "ë§¹í™œì•½", "ì¾Œíˆ¬", "ë¬´ì‹¤ì ", "ì„¸ì´ë¸Œ", "QS", "í”¼ì¹­",
        "3ì—°ìŠ¹", "4ì—°ìŠ¹", "ê²½ê¸° ìŠ¹ë¦¬", "ì‹œì¦Œ ì²« ìŠ¹", "ì²« ìŠ¹ë¦¬", "íƒ€ì ", "ìˆ˜í›ˆ",
        "ê¸°ë¡ ê²½ì‹ ", "ì‹ ê¸°ë¡", "ì„ ë°œ ì¶œì „", "íƒ€ê²©ê°", "ì¢‹ì€ ê²½ê¸°ë ¥", "ì •ìƒ ë³µê·€",
        "ë³µê·€ì „ ìŠ¹ë¦¬", "ìŠ¹ë¶€ì²˜ ì¥ì•…", "ê¸°ì„¸", "ì—ì´ìŠ¤", "ì£¼ì „ ë³µê·€", "ì²« í™ˆëŸ°"
    ]

    negative_words = [
        "íŒ¨ë°°", "ì—­ì „íŒ¨", "ì™„íŒ¨", "ì°¸íŒ¨", "ë¬´ë“ì ", "ì‹¤ì±…", "ë³‘ì‚´íƒ€", "ë¶€ìƒ", "ì´íƒˆ",
        "ê²°ì¥", "ë‚™ë§ˆ", "ë¶€ì§„", "ë¶€ì§„í•œ", "ë¶ˆì•ˆ", "ì‹¤ë§", "íƒˆë½", "ë¬´ê¸°ë ¥",
        "íƒ€ê²© ì¹¨ë¬µ", "íƒ€ì„  ì¹¨ë¬µ", "ì„±ì  ë¶€ì§„", "ERA ìƒìŠ¹", "íƒ€ìœ¨ í•˜ë½",
        "ì¡°ê¸° ê°•íŒ", "ë³¼ë„·", "ë‚œì¡°", "ë‚œíƒ€", "ì‹¤ì ", "ê¸°íšŒ ë¬´ì‚°", "ì£¼ë£¨ì‚¬",
        "ì—°íŒ¨", "3ì—°íŒ¨", "4ì—°íŒ¨", "5ì—°íŒ¨", "1êµ° ì œì™¸", "ë“±ë¡ ë§ì†Œ", "ë°©ë§ì´ë„ ì¡ì§€ ëª»í•´",
        "ì—”íŠ¸ë¦¬ ì œì™¸", "ëª»í•˜ëŠ”", "í—ˆìš©", "ë¶ˆíœ ë‚œì¡°", "ì„ ë°œ ë¬´ë„ˆì§", "ë¬´ìŠ¹ë¶€",
        "ì•„ì‰¬ìš´", "ì´ˆë°˜ì— ë°€ë ¸ë‹¤", "íŒì • ë…¼ë€", "ì§•ê³„", "ì¶œì „ ì •ì§€"
    ]

    for idx, article in enumerate(articles):
        st.markdown(f"### ğŸ“° ê¸°ì‚¬ #{idx+1}")
        st.text(article)

        try:
            label, pos_score, neg_score = sa.predict(article)
        except Exception as e:
            st.error(f"ê°ì • ë¶„ì„ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

        translated_label = label_map.get(label, label)

        # ê²°ê³¼ ì¶œë ¥
        st.write(f"**ê°ì • ë¶„ì„ ê²°ê³¼:** {translated_label}")
        st.write(f"ê¸ì •: {pos_score:.4f} / ë¶€ì •: {neg_score:.4f}")
        st.progress(pos_score)
        st.caption("âš ï¸ ê°ì • ë¶„ì„ì€ ì¼ë°˜ í…ìŠ¤íŠ¸ ê¸°ë°˜ì´ë©°, ìŠ¤í¬ì¸  ê¸°ì‚¬ì—ì„œëŠ” ì‹¤ì œ ë§¥ë½ê³¼ ë‹¤ë¥´ê²Œ ë¶„ë¥˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        # ìŠ¤í¬ì¸  í‚¤ì›Œë“œ ê¸°ë°˜ ë³´ì • ë©”ì‹œì§€ (label ìì²´ëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ)
        has_positive = any(word in article for word in positive_words)
        has_negative = any(word in article for word in negative_words)

        if has_negative:
            st.caption("âš ï¸ ë¶€ì •ì ì¸ ìŠ¤í¬ì¸  í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆì–´ ê°ì • ê²°ê³¼ê°€ ë³´ì •ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif label == "Negative" and has_positive:
            st.caption("âœ… ê¸ì •ì ì¸ ìŠ¤í¬ì¸  í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆì–´ ê°ì • ê²°ê³¼ê°€ ë³´ì •ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        sentiment_counts[translated_label] += 1

        # ê°œì²´ëª… ì¸ì‹
        try:
            entities = ke.extract(article)
            st.write("ğŸ“ **ê°œì²´ëª… ì¶”ì¶œ ê²°ê³¼:**")
            if entities["PER"]:
                st.markdown(f"- ì„ ìˆ˜: {', '.join(set(entities['PER']))}")
            if entities["ORG"]:
                st.markdown(f"- íŒ€: {', '.join(set(entities['ORG']))}")
            if entities["RECORD"]:
                st.markdown(f"- ê¸°ë¡: {', '.join(set(entities['RECORD']))}")

            # í‚¤ì›Œë“œ ëˆ„ì  ì €ì¥ (ì„ ìˆ˜ëª… + íŒ€ëª… ê¸°ë°˜)
            keywords = dict(Counter(entities["PER"] + entities["ORG"]))
            all_keywords.update(keywords)

        except Exception as e:
            st.error(f"ê°œì²´ëª… ì¶”ì¶œ ì˜¤ë¥˜ ë°œìƒ: {e}")

        st.markdown("---")

    st.info("â„¹ï¸ ì—¬ëŸ¬ ê¸°ì‚¬ë¥¼ ë„£ìœ¼ë ¤ë©´ ê¸°ì‚¬ ì‚¬ì´ì— **ë¹ˆ ì¤„ 5ì¹¸ ì´ìƒ** (Enter 5ë²ˆ)ì„ ë„£ì–´ì£¼ì„¸ìš”!")

    # âœ… í•œê¸€ í°íŠ¸ ì„¤ì •
    font_path = "NanumGothic.ttf"
    font_prop = fm.FontProperties(fname=font_path)

    st.subheader("ğŸ“Š ê°ì • ë¶„ì„ ìš”ì•½")

    labels = ["ê¸ì •", "ë¶€ì •"]
    values = [sentiment_counts["ê¸ì •"], sentiment_counts["ë¶€ì •"]]
    colors = ["#4da6ff", "#ff6666"]

    sns.set_style("whitegrid")
    fig, ax = plt.subplots(figsize=(6, 4))
    bars = ax.bar(labels, values, color=colors)

    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width() / 2, height + 0.1, f"{int(height)}",
                ha='center', va='bottom', fontsize=12, fontweight='bold', fontproperties=font_prop)

    ax.set_xticklabels(labels, fontproperties=font_prop)
    ax.set_ylim(0, max(values) + 1)
    ax.set_ylabel("ê°ì •ë³„ ê¸°ì‚¬ ê°œìˆ˜", fontsize=11, fontproperties=font_prop)
    ax.set_xlabel("ê°ì • ë¶„ë¥˜", fontsize=11, fontproperties=font_prop)
    ax.set_title("ê°ì • ë¶„ì„ ê²°ê³¼ ë¶„í¬", fontsize=14, fontweight='bold', fontproperties=font_prop)

    st.pyplot(fig)

    # â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ
    st.subheader("â˜ï¸ í‚¤ì›Œë“œ ì›Œë“œ í´ë¼ìš°ë“œ")
    if all_keywords:
        wc = WordCloud(
            font_path=font_path,
            background_color="white",
            width=800,
            height=400
        )
        wc.generate_from_frequencies(all_keywords)

        fig2, ax2 = plt.subplots(figsize=(10, 5))
        ax2.imshow(wc, interpolation="bilinear")
        ax2.axis("off")
        st.pyplot(fig2)
    else:
        st.write("â— í‚¤ì›Œë“œê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
